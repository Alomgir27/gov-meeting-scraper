# Government Meeting Scraper

Robust web scraper for extracting meeting metadata from government websites with automatic retry, bot detection avoidance, and incremental saving.

**Key Features:** Sequential processing â€¢ Incremental saving â€¢ Automatic retry â€¢ Bot avoidance â€¢ Rate limiting â€¢ Zero false positives

---

## ğŸ“¦ Setup

**Prerequisites:** Python 3.8+

**Compatible with:** Windows (Git Bash), Linux, Mac

```bash
bash setup.sh
```

This installs dependencies, Playwright browser, and creates necessary directories.

The scripts automatically detect your OS and configure accordingly.

---

## ğŸš€ Usage

### Problem 1: Scrape Meeting Metadata

```bash
bash run.sh
```

**Output:** `outputs/problem1_complete_output.json`

Scrapes 6 government websites (Nov 20, 2024 - Nov 26, 2025)

### Problem 2: Resolve Video/Audio/Document URLs

```bash
bash run.sh problem2
```

**Output:** `outputs/problem2_output.json`

Resolves and verifies 11 URLs from assignment:
- âœ… yt-dlp --simulate verification (videos/audio)
- âœ… HTTP HEAD verification (documents)
- âœ… Auto-retry network failures (2-3 attempts)
- âœ… Platform transformations (Swagit /download)

**Supported:** YouTube, IBM Video, Granicus, ChampDS, Viebit, SharePoint, Audiomack, PDF, HTML

### Custom Input (Problem 1)

```bash
python scraper.py scrape-meetings -i inputs/custom.json -o outputs/result.json
```

Input format:
```json
{
  "start_date": "2024-01-01",
  "end_date": "2024-12-31",
  "base_urls": ["https://your-site.gov/meetings"]
}
```

### Custom Input (Problem 2)

```bash
python scraper.py resolve-urls -i inputs/custom_urls.json -o outputs/resolved.json
```

Input format:
```json
[
  {"url": "https://example.com/video.mp4", "type": "video"},
  {"url": "https://example.com/doc.pdf", "type": "document"}
]
```

---

## ğŸ“‹ Input/Output Formats

### Problem 1

**Input:**
```json
{
  "start_date": "2024-11-20",
  "end_date": "2025-11-26",
  "base_urls": ["https://example.gov/meetings"]
}
```

**Output:**
```json
[{
  "base_url": "https://example.gov/meetings",
  "medias": [{
    "meeting_url": "https://youtube.com/watch?v=...",
    "agenda_url": "https://example.gov/agenda.pdf",
    "minutes_url": "https://example.gov/minutes.pdf",
    "title": "City Council Meeting",
    "date": "2024-11-20"
  }]
}]
```

### Problem 2

**Input:**
```json
[
  {"url": "https://swagit.com/videos/123", "type": "audio"},
  {"url": "https://example.gov/doc.pdf", "type": "document"}
]
```

**Output:**
```json
[
  "https://swagit.com/videos/123/download",
  "https://example.gov/doc.pdf"
]
```

---

## ğŸ“ Project Structure

```
scraping/
â”œâ”€â”€ scraper.py              # CLI interface
â”œâ”€â”€ setup.sh                # Installation script
â”œâ”€â”€ run.sh                  # Quick run (Problem 1 & 2)
â”œâ”€â”€ inputs/                 # Input JSON files
â”‚   â”œâ”€â”€ problem1_all_domains.json
â”‚   â””â”€â”€ problem2_input.json
â”œâ”€â”€ outputs/                # Generated outputs (incremental saves)
â”‚   â”œâ”€â”€ problem1_complete_output.json
â”‚   â””â”€â”€ problem2_output.json
â”œâ”€â”€ logs/                   # Application logs
â””â”€â”€ src/
    â”œâ”€â”€ core/               # Core services
    â”‚   â”œâ”€â”€ engine.py       # Main orchestrator
    â”‚   â”œâ”€â”€ browser.py      # Browser management
    â”‚   â”œâ”€â”€ stealth.py      # Anti-detection
    â”‚   â””â”€â”€ url_resolver.py # URL verification (Problem 2)
    â”œâ”€â”€ extractors/         # Site-specific extraction
    â”‚   â”œâ”€â”€ base_extractor.py
    â”‚   â”œâ”€â”€ site_handlers.py
    â”‚   â””â”€â”€ site_specific/  # Individual site handlers
    â”œâ”€â”€ storage/            # Data models & persistence
    â”‚   â”œâ”€â”€ models.py
    â”‚   â”œâ”€â”€ meeting_models.py
    â”‚   â””â”€â”€ writer.py
    â””â”€â”€ utils/              # Helpers & logging
        â”œâ”€â”€ logger.py
        â”œâ”€â”€ helpers.py
        â””â”€â”€ error_detector.py
```

---

## ğŸ”§ Configuration

Edit `src/storage/models.py` and `src/core/url_resolver.py` for custom settings.

**Key settings:**
- Rate limit: 2 req/sec per domain
- Retries: 2-3 attempts for network errors
- Timeouts: 20-45s depending on operation

---

## ğŸ› Troubleshooting

**Virtual environment not found:**
```bash
bash setup.sh
```

**Browser missing:**
```bash
playwright install chromium
```

**Python not found:**
- Windows: Install from python.org
- Linux: `sudo apt install python3 python3-pip`
- Mac: `brew install python3`

---

## ğŸ¯ Assignment Coverage

âœ… **Problem 1**: Meeting metadata scraping (6 domains)  
âœ… **Problem 2**: URL resolution with retry logic (11 URLs)  
âœ… **Bonus Task**: Universal scraper (40+ sites)

---

## ğŸ—ï¸ Architecture

<div align="center">

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       User Input                             â”‚
â”‚              (JSON with URLs & Date Range)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    scraper.py (CLI)                          â”‚
â”‚              Command-line interface handler                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ScraperEngine (core/engine.py)              â”‚
â”‚   â€¢ Orchestrates scraping workflow                          â”‚
â”‚   â€¢ Manages browser pool & HTTP requests                    â”‚
â”‚   â€¢ Handles retries & rate limiting                         â”‚
â”‚   â€¢ Sequential processing with callbacks                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                               â”‚
         â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Browser Manager    â”‚â”€â”€â”€â”€â”€â–¶â”‚   Site-Specific          â”‚
â”‚   (core/browser.py)  â”‚      â”‚   Extractors             â”‚
â”‚                      â”‚      â”‚   (extractors/*)         â”‚
â”‚ â€¢ Playwright setup   â”‚      â”‚                          â”‚
â”‚ â€¢ Stealth mode       â”‚      â”‚ â€¢ Uses browser to load   â”‚
â”‚ â€¢ Anti-detection     â”‚      â”‚ â€¢ Detects site type      â”‚
â”‚ â€¢ Auto-scroll        â”‚      â”‚ â€¢ Extracts meetings      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ â€¢ Parses dates           â”‚
                              â”‚ â€¢ Classifies links       â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                           â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚    Data Validation      â”‚
                              â”‚                         â”‚
                              â”‚ â€¢ Date range filter     â”‚
                              â”‚ â€¢ URL verification      â”‚
                              â”‚ â€¢ Duplicate removal     â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                           â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚   Incremental Save      â”‚
                              â”‚                         â”‚
                              â”‚ â€¢ Save after each domainâ”‚
                              â”‚ â€¢ Progress tracking     â”‚
                              â”‚ â€¢ No data loss          â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                           â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚   JSON Output           â”‚
                              â”‚   (outputs/*.json)      â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</div>

---

**Note:** For ethical scraping of public government data only.
