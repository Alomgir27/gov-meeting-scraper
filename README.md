# Government Meeting Scraper

Robust web scraper for extracting meeting metadata from government websites with automatic retry, bot detection avoidance, and incremental saving.

**Key Features:** Sequential processing â€¢ Incremental saving â€¢ Automatic retry â€¢ Bot avoidance â€¢ Rate limiting â€¢ Zero false positives

---

## ğŸ“¦ Setup

**Prerequisites:** Python 3.8+

**Compatible with:** Windows (Git Bash), Linux, Mac

```bash
chmod +x setup.sh
bash setup.sh
```

This installs dependencies, Playwright browser, and creates necessary directories.

The scripts automatically detect your OS and configure accordingly.

---

## ğŸš€ Usage

### Run All Tasks

```bash
chmod +x run.sh
bash run.sh
```

Runs Problem 1 â†’ Problem 2 â†’ Bonus sequentially

### Problem 1: Meeting Metadata

```bash
chmod +x run.sh
bash run.sh problem1
```

**Output:** `outputs/problem1_complete_output.json`

Scrapes 6 government websites (Nov 20, 2024 - Nov 26, 2025)

### Problem 2: URL Resolution

```bash
chmod +x run.sh
bash run.sh problem2
```

**Output:** `outputs/problem2_output.json`

Resolves and verifies 11 URLs:
- yt-dlp --simulate (videos/audio)
- HTTP HEAD (documents)
- Platform transformations (Swagit /download)

**Supported:** YouTube, IBM Video, Granicus, ChampDS, Viebit, SharePoint, Audiomack, PDF, HTML

### Bonus: Universal Scraper

```bash
chmod +x run.sh
bash run.sh bonus
```

**Output:** `outputs/bonus_output.json`

One scraper for 40 diverse sites â€¢ Auto-detect patterns â€¢ 100% accuracy (zero false positives)

### Custom Input

**Problem 1:**
```bash
python scraper.py scrape-meetings -i inputs/custom.json -o outputs/result.json
```

**Problem 2:**
```bash
python scraper.py resolve-urls -i inputs/custom_urls.json -o outputs/resolved.json
```

---

## ğŸ“‹ Input/Output Formats

### Problem 1

**Input:**
```json
{
  "start_date": "2024-11-20",
  "end_date": "2025-11-26",
  "base_urls": ["https://example.gov/meetings"]
}
```

**Output:**
```json
[{
  "base_url": "https://example.gov/meetings",
  "medias": [{
    "meeting_url": "https://youtube.com/watch?v=...",
    "agenda_url": "https://example.gov/agenda.pdf",
    "minutes_url": "https://example.gov/minutes.pdf",
    "title": "City Council Meeting",
    "date": "2024-11-20"
  }]
}]
```

### Problem 2

**Input:**
```json
[
  {"url": "https://swagit.com/videos/123", "type": "audio"},
  {"url": "https://example.gov/doc.pdf", "type": "document"}
]
```

**Output:**
```json
[
  "https://swagit.com/videos/123/download",
  "https://example.gov/doc.pdf"
]
```

---

## ğŸ“ Project Structure

```
scraping/
â”œâ”€â”€ scraper.py              # CLI interface
â”œâ”€â”€ setup.sh                # Installation script
â”œâ”€â”€ run.sh                  # Quick run (Problem 1 & 2)
â”œâ”€â”€ inputs/                 # Input JSON files
â”‚   â”œâ”€â”€ problem1_all_domains.json
â”‚   â””â”€â”€ problem2_input.json
â”œâ”€â”€ outputs/                # Generated outputs (incremental saves)
â”‚   â”œâ”€â”€ problem1_complete_output.json
â”‚   â””â”€â”€ problem2_output.json
â”œâ”€â”€ logs/                   # Application logs
â””â”€â”€ src/
    â”œâ”€â”€ core/               # Core services
    â”‚   â”œâ”€â”€ engine.py       # Main orchestrator
    â”‚   â”œâ”€â”€ browser.py      # Browser management
    â”‚   â”œâ”€â”€ stealth.py      # Anti-detection
    â”‚   â””â”€â”€ url_resolver.py # URL verification (Problem 2)
    â”œâ”€â”€ extractors/         # Site-specific extraction
    â”‚   â”œâ”€â”€ base_extractor.py
    â”‚   â”œâ”€â”€ site_handlers.py
    â”‚   â””â”€â”€ site_specific/  # Individual site handlers
    â”œâ”€â”€ storage/            # Data models & persistence
    â”‚   â”œâ”€â”€ models.py
    â”‚   â”œâ”€â”€ meeting_models.py
    â”‚   â””â”€â”€ writer.py
    â””â”€â”€ utils/              # Helpers & logging
        â”œâ”€â”€ logger.py
        â”œâ”€â”€ helpers.py
        â””â”€â”€ error_detector.py
```

---

## ğŸ”§ Configuration

Edit `src/storage/models.py` and `src/core/url_resolver.py` for custom settings.

**Key settings:**
- Rate limit: 2 req/sec per domain
- Retries: 2-3 attempts for network errors
- Timeouts: 20-45s depending on operation

---

## ğŸ› Troubleshooting

**Virtual environment not found:**
```bash
bash setup.sh
```

**Browser missing:**
```bash
playwright install chromium
```

**Python not found:**
- Windows: Install from python.org
- Linux: `sudo apt install python3 python3-pip`
- Mac: `brew install python3`

---

## ğŸ¯ Assignment Coverage

âœ… **Problem 1**: Meeting metadata scraping (6 domains)  
âœ… **Problem 2**: URL resolution with retry logic (11 URLs)  
âœ… **Bonus Task**: Universal scraper (40+ sites)

---

## ğŸ—ï¸ Architecture & Workflow

### Core Components

**scraper.py** â†’ **ScraperEngine** â†’ **Browser + Extractors** â†’ **Output (JSON)**

- **ScraperEngine**: Orchestration, retry logic, rate limiting
- **BrowserManager**: Playwright with stealth mode (anti-detection)
- **Extractors**: Site-specific handlers + Universal fallback
- **URLResolver**: Media verification with yt-dlp

### Workflow

```mermaid
graph TD
    A[Input JSON<br/>dates + URLs] --> B[CLI scraper.py<br/>Routes: Problem 1 or 2]
    B --> C[ScraperEngine<br/>Rate limit: 2 req/sec<br/>Sequential + Retry]
    C --> D[Browser Stealth Mode<br/>Load JS + Anti-detection]
    D --> E[Smart Extraction<br/>Site-specific â†’ Universal fallback]
    E --> F[Validation & Filter<br/>Date range + Dedup]
    F --> G[Save Progress<br/>After each domain]
    G --> H[Output JSON]
```

---

**Note:** For ethical scraping of public government data only.
